{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-03: Tweaking chunking strategy of Ragas and reviewing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "In Lab-01 and Lab-02 you created an Amazon Bedrock Knowledge Base to power your RAG applicaiton. Then, you evaluated it using the RAGAS framework against different RAGAS metics and powering your RAG with different Large Language Models (llms).\n",
    "\n",
    "In this notebook, we will work on the Amazon Bedrock Knowledge Base to observe how to improve the RAGAS metrics by changing the document chunking strategies.\n",
    "\n",
    "### Chunking Introduction\n",
    "\n",
    "Chunking is a critical step in building an effective knowledge base for RAG applications. The choice of chunking strategy can significantly improve or lower your RAG implementation's performance. Here's why:\n",
    "\n",
    "1. Retrieval accuracy: Different chunking methods can lead to varying levels of precision in retrieving relevant information. An optimal chunking strategy ensures that the most pertinent information is captured in each chunk, improving the chances of retrieving the right context for a given query.\n",
    "\n",
    "2. Context quality: The size and content of chunks directly affect the quality of context provided to the language model. Too large chunks may include irrelevant information, while too small chunks might miss important context. Finding the right balance is crucial for generating accurate and relevant responses.\n",
    "\n",
    "3. Computational efficiency: Chunking strategies impact the number and size of vectors in your knowledge base. This, in turn, affects the computational resources required for embedding generation and similarity search. An efficient chunking strategy can lead to faster retrieval times and lower resource consumption.\n",
    "\n",
    "4. Adaptability to content: Different types of documents (e.g., technical reports, narratives, or structured data) may benefit from different chunking approaches. The ability to tailor your chunking strategy to your specific content can significantly enhance your RAG system's performance.\n",
    "\n",
    "<!-- ![retrieveapi.png](./images/retrieveAPI.png) -->\n",
    "<img src=\"./assets/retrieveAPI.png\" width=50% height=20% />\n",
    "\n",
    "In the RAG workflow, chunking impacts the \"Retrieve API\" stage, as shown in the provided image. The process involves:\n",
    "\n",
    "1. Generating query embeddings from the user input.\n",
    "2. Retrieving similar documents (chunks) from the knowledge base.\n",
    "3. Using these retrieved chunks as context for prompt augmentation.\n",
    "\n",
    "It's important to note that there is no one-size-fits-all solution for chunking. The optimal strategy often depends on the nature of your documents, the specifics of your use case, and the characteristics of your target queries. This is why testing different chunking strategies and evaluating their impact on your RAG system's performance is crucial.\n",
    "\n",
    "### Amazon Bedrock Knowledge Bases Chunking Strategies\n",
    "\n",
    "Amazon Bedrock offers several chunking strategies to optimize your knowledge base for different types of content and use cases:\n",
    "\n",
    "1. Standard Chunking:\n",
    "   - Fixed-size chunking: Allows you to specify the number of tokens per chunk and an overlap percentage.\n",
    "   - Default chunking: Splits content into approximately 300-token chunks, preserving sentence boundaries.\n",
    "   - Pros:\n",
    "       - Simple and straightforward to implement\n",
    "       - Works well for uniform, well-structured documents\n",
    "       - Overlap feature helps maintain context across chunk boundaries\n",
    "   - Cons:\n",
    "       - May split semantic units or important context\n",
    "       - Less effective for documents with varying content density or structure\n",
    "       - Fixed-size approach might not adapt well to diverse document types\n",
    "\n",
    "3. Hierarchical Chunking:\n",
    "   - Creates a two-level structure with parent and child chunks.\n",
    "   - You can set maximum token sizes for both parent and child chunks, as well as overlap tokens.\n",
    "   - Balances precision (small child chunks) with comprehensive context (larger parent chunks).\n",
    "   - Pros:\n",
    "       - Preserves both local and broader context\n",
    "       - Allows for more nuanced retrieval (e.g., returning child chunks with parent context)\n",
    "       - Can improve performance for documents with clear hierarchical structure\n",
    "   - Cons:\n",
    "       - More complex to set up and fine-tune\n",
    "       - May introduce overhead in storage and retrieval processes\n",
    "       - Might not be beneficial for flat or unstructured documents\n",
    "\n",
    "4. Semantic Chunking:\n",
    "   - Uses natural language processing to create meaningful chunks based on semantic content.\n",
    "   - Configurable parameters include maximum tokens, buffer size, and breakpoint percentile threshold.\n",
    "   - Aims to improve retrieval accuracy by focusing on semantic rather than just syntactic structure.\n",
    "   - Pros:\n",
    "       - Creates more meaningful and coherent chunks based on content\n",
    "       - Can significantly improve retrieval relevance for complex documents\n",
    "       - Adapts to varying content density within documents\n",
    "   - Cons:\n",
    "       - More computationally intensive during ingestion\n",
    "       - May require more fine-tuning to achieve optimal results\n",
    "       - Performance can vary depending on the effectiveness of the underlying NLP model\n",
    "\n",
    "5. Advanced Parsing Options:\n",
    "   - Utilizes foundation models (like Claude 3 Sonnet or Claude 3 Haiku) for parsing complex data such as tables and charts.\n",
    "   - Allows customization of parsing prompts for specific use cases.\n",
    "   - Pros:\n",
    "       - Excellent for handling complex, structured data like tables and charts\n",
    "       - Allows for customization to specific document types or domains\n",
    "       - Can significantly improve accuracy for specialized content\n",
    "   - Cons:\n",
    "       - Requires more setup and potentially ongoing maintenance\n",
    "       - May be overkill for simpler document types\n",
    "       - Dependent on the capabilities of the chosen foundation model\n",
    "\n",
    "6. Custom Transformation:\n",
    "   - Enables the use of a Lambda function to implement custom chunking logic.\n",
    "   - Useful for specific chunking requirements not natively supported by Amazon Bedrock.\n",
    "   - Pros:\n",
    "       - Offers maximum flexibility for unique document structures or use cases\n",
    "       - Allows integration of domain-specific knowledge into the chunking process\n",
    "       - Can be optimized for specific performance requirements\n",
    "   - Cons:\n",
    "       - Requires custom development and maintenance\n",
    "       - May introduce complexity and potential points of failure\n",
    "       - Can be challenging to scale or adapt to changing requirements\n",
    "\n",
    "It's crucial to emphasize that the effectiveness of these chunking strategies can vary significantly depending on your specific use case, document types, and query patterns. Testing different strategies and carefully evaluating their impact on your RAGAS metrics is essential for optimizing your RAG implementation. This notebook will guide you through this process, helping you understand how different chunking approaches affect the performance of your knowledge base for the Octank financial reports dataset.\n",
    "\n",
    "\n",
    "### Notebook Walkthrough\n",
    "\n",
    "In this notebook we will create different Knowledge Bases with different chunking strategies based on the same dataset of previous labs.\n",
    "Then, we will test each Knowledge Base created with RAGAS using `anthropic.claude-3-5-sonnet-20240620-v1:0` as evaluator.\n",
    "\n",
    "In the end, we will be able to observe that different chunking strategies impact the RAGAS metrics.\n",
    "\n",
    "### Python 3.10\n",
    "\n",
    "⚠  For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠\n",
    "\n",
    "### Setup\n",
    "\n",
    "To run this notebook you would need to install dependencies, langchain and RAGAS and the updated boto3, botocore whls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3 --quiet\n",
    "%pip install botocore --quiet\n",
    "%pip install langchain>0.1 --quiet\n",
    "%pip install ragas==0.1.9 --quiet\n",
    "%pip install opensearch-py --quiet\n",
    "%pip install retrying==1.3.4 --quiet\n",
    "%pip install langchain-aws --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Knowledge Bases with Different Chunking Strategies\n",
    "\n",
    "In this section, we will create two different knowledge bases and reuse one of the knowledge bases created in Lab 1. Each knowledge base will use a different chunking strategy to process the same dataset and we will compare their performance using Ragas. \n",
    "\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Create Knowledge Base 1: Hierarchical Transformation\n",
    "   - Set maximum token size of parent to 1024 (max size of cohere embedding model)\n",
    "   - Set maximum token size of child to 300\n",
    "   - Set overlap tokens to 60\n",
    "\n",
    "2. Create Knowledge Base 2: Semantic Transformation\n",
    "   - Set breakpoint percentile threshold to 55\n",
    "   - Set buffer size to 1\n",
    "   - Set maximum tokens to 300\n",
    "\n",
    "3. Reuse Knowledge Base from Lab 1: Fixed Chunking\n",
    "   - Rerun the knowledge base creation function to reuse the already created knowledge base\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- Ensure you're using the same embedding model (Cohere) for all three knowledge bases to maintain consistency in our comparison.\n",
    "- The Knowledge Base IDs will be crucial for our subsequent notebook operations, so make sure to record them accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets double check our data is uploaded to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "# Specify your bucket to be the default sagemaker bucket\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() #sagemaker-abcdef\n",
    "filename = 'octank_financial_10K.pdf'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload the file\n",
    "s3.upload_file(filename, bucket, filename)\n",
    "pp.pprint(f\"Upload Successful: {filename} uploaded to {bucket}/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create hierarchical and semantic knowledge bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from utility import interactive_sleep, create_knowledge_base, create_ds\n",
    "\n",
    "\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "body_json = {\n",
    "       \"settings\": {\n",
    "          \"index.knn\": \"true\",\n",
    "           \"number_of_shards\": 1,\n",
    "           \"knn.algo_param.ef_search\": 512,\n",
    "           \"number_of_replicas\": 0,\n",
    "       },\n",
    "       \"mappings\": {\n",
    "          \"properties\": {\n",
    "             \"vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                 \"method\": {\n",
    "                     \"name\": \"hnsw\",\n",
    "                     \"engine\": \"faiss\",\n",
    "                     \"space_type\": \"l2\"\n",
    "                 },\n",
    "             },\n",
    "             \"text\": {\n",
    "                \"type\": \"text\"\n",
    "             },\n",
    "             \"text-metadata\": {\n",
    "                \"type\": \"text\"         }\n",
    "          }\n",
    "       }\n",
    "    }\n",
    "\n",
    "## create new hierarchical knowledge base\n",
    "cohere_embed_hierarchical_knowledge_base = create_knowledge_base(\n",
    "    index_name=\"cohere-embed-english-v3\", \n",
    "    body_json=body_json, \n",
    "    collection_name=\"cohere-embed-english-v3\", \n",
    "    knowledge_base_name=\"cohere-embed-hierarchical-english-v3\",\n",
    "    vector_store_name=\"cohere-embed-english-v3\",\n",
    "    access_policy_name=\"cohere-embed-access-policy\",\n",
    "    embedding_model_arn=f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-english-v3\"\n",
    "    )\n",
    "\n",
    "## create new semantic knowledge base\n",
    "cohere_embed_semantic_knowledge_base = create_knowledge_base(\n",
    "    index_name=\"cohere-embed-english-v3\", \n",
    "    body_json=body_json, \n",
    "    collection_name=\"cohere-embed-english-v3\", \n",
    "    knowledge_base_name=\"cohere-embed-semantic-english-v3\",\n",
    "    vector_store_name=\"cohere-embed-english-v3\",\n",
    "    access_policy_name=\"cohere-embed-access-policy\",\n",
    "    embedding_model_arn=f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-english-v3\"\n",
    "    )\n",
    "\n",
    "## reuse existing fixed embedding knowledge base. \n",
    "## Don't worry, this will not create a new one and will just return the old knowledge base created in Lab 1\n",
    "cohere_embed_fixed_knowledge_base = create_knowledge_base(\n",
    "    index_name=\"cohere-embed-english-v3\", \n",
    "    body_json=body_json, \n",
    "    collection_name=\"cohere-embed-english-v3\", \n",
    "    knowledge_base_name=\"cohere-embed-english-v3\",\n",
    "    vector_store_name=\"cohere-embed-english-v3\",\n",
    "    access_policy_name=\"cohere-embed-access-policy\",\n",
    "    embedding_model_arn=f\"arn:aws:bedrock:{region_name}::foundation-model/cohere.embed-english-v3\"\n",
    "    )\n",
    "\n",
    "pp.pprint(f\"Cohere Embed Hierarchical Knowledge Base Id: {cohere_embed_hierarchical_knowledge_base['knowledgeBaseId']}\")\n",
    "pp.pprint(f\"Cohere Embed Semantic Knowledge Base Id: {cohere_embed_semantic_knowledge_base['knowledgeBaseId']}\")\n",
    "pp.pprint(f\"Cohere Embed Fixed Knowledge Base Id: {cohere_embed_fixed_knowledge_base['knowledgeBaseId']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Sources and Sync\n",
    "\n",
    "Steps:\n",
    "\n",
    "* determine chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the chunkingStrategyConfiguration.\n",
    "* initialize the s3 configuration in order to create the data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchicalChunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"HIERARCHICAL\", \n",
    "    \"hierarchicalChunkingConfiguration\": {\n",
    "        \"levelConfigurations\": [{\"maxTokens\": 1024}, {\"maxTokens\": 300}],\n",
    "        \"overlapTokens\": 60\n",
    "    }\n",
    "}\n",
    "\n",
    "semanticChunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"SEMANTIC\", \n",
    "    \"semanticChunkingConfiguration\": {\n",
    "        \"breakpointPercentileThreshold\": 55,\n",
    "        \"bufferSize\": 1,\n",
    "        \"maxTokens\": 300\n",
    "    }\n",
    "}\n",
    "\n",
    "s3DataSourceConfiguration = {\n",
    "    \"type\": \"S3\",\n",
    "    \"s3Configuration\": {\n",
    "        \"bucketArn\": \"\",\n",
    "        \"inclusionPrefixes\":[\"octank_financial_10K.pdf\"] \n",
    "    }\n",
    "}\n",
    "\n",
    "data_sources=[\n",
    "                {\"type\": \"S3\", \"bucket_name\": bucket} \n",
    "            ]\n",
    "cohere_embed_hierarchical_data_source = create_ds(data_sources, hierarchicalChunkingStrategyConfiguration, s3DataSourceConfiguration, cohere_embed_hierarchical_knowledge_base['knowledgeBaseId'])\n",
    "cohere_embed_semantic_data_source = create_ds(data_sources, semanticChunkingStrategyConfiguration, s3DataSourceConfiguration, cohere_embed_semantic_knowledge_base['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow the steps below to set up necessary packages\n",
    "\n",
    "1. Import the necessary libraries for creating `bedrock-runtime` for invoking foundation models and `bedrock-agent-runtime` client for using Retrieve API provided by Knowledge Bases for Amazon Bedrock. \n",
    "2. Import Langchain for: \n",
    "   1. Initializing bedrock model  `anthropic.claude-3-haiku-20240307-v1:0` as our large language model to perform query completions using the RAG pattern. \n",
    "   2. Initializing bedrock model  `anthropic.claude-3-sonnet-20240229-v1:0` as our large language model to perform RAG evaluation. \n",
    "   3. Initialize Langchain retriever integrated with knowledge bases. \n",
    "   4. Later in the notebook we will wrap the LLM and retriever with `RetrieverQAChain` for building our Q&A application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "CredentialRetrievalError",
     "evalue": "Error when retrieving credentials from custom-process: You need to authenticate with Midway. \nRun the following command before retrying: mwinit --aea\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCredentialRetrievalError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m kb_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY9WCUTR89N\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Replace with your knowledge base id here.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m bedrock_config \u001b[38;5;241m=\u001b[39m Config(connect_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, read_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, retries\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_attempts\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[0;32m---> 15\u001b[0m bedrock_client \u001b[38;5;241m=\u001b[39m \u001b[43mboto3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbedrock-runtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m bedrock_agent_client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedrock-agent-runtime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m                               config\u001b[38;5;241m=\u001b[39mbedrock_config\n\u001b[1;32m     18\u001b[0m                               )\n\u001b[1;32m     20\u001b[0m llm_for_text_generation \u001b[38;5;241m=\u001b[39m BedrockChat(model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic.claude-3-haiku-20240307-v1:0\u001b[39m\u001b[38;5;124m\"\u001b[39m, client\u001b[38;5;241m=\u001b[39mbedrock_client)\n",
      "File \u001b[0;32m~/git/reinvent_2024_stuff/ragas-lab/.venv/lib/python3.12/site-packages/boto3/__init__.py:92\u001b[0m, in \u001b[0;36mclient\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclient\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Create a low-level service client by name using the default session.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    See :py:meth:`boto3.session.Session.client`.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_default_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/reinvent_2024_stuff/ragas-lab/.venv/lib/python3.12/site-packages/boto3/session.py:297\u001b[0m, in \u001b[0;36mSession.client\u001b[0;34m(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclient\u001b[39m(\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    217\u001b[0m     service_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    227\u001b[0m ):\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    Create a low-level service client by name.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m \n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mservice_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ssl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ssl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43maws_access_key_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_access_key_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43maws_secret_access_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_secret_access_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43maws_session_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_session_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/reinvent_2024_stuff/ragas-lab/.venv/lib/python3.12/site-packages/botocore/session.py:957\u001b[0m, in \u001b[0;36mSession.create_client\u001b[0;34m(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PartialCredentialsError(\n\u001b[1;32m    951\u001b[0m         provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicit\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    952\u001b[0m         cred_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_cred_vars(\n\u001b[1;32m    953\u001b[0m             aws_access_key_id, aws_secret_access_key\n\u001b[1;32m    954\u001b[0m         ),\n\u001b[1;32m    955\u001b[0m     )\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 957\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m auth_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_auth_token()\n\u001b[1;32m    959\u001b[0m endpoint_resolver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_internal_component(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendpoint_resolver\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/git/reinvent_2024_stuff/ragas-lab/.venv/lib/python3.12/site-packages/botocore/session.py:515\u001b[0m, in \u001b[0;36mSession.get_credentials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03mReturn the :class:`botocore.credential.Credential` object\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;124;03massociated with this session.  If the credentials have not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_components\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_component\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcredential_provider\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m--> 515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials\n",
      "File \u001b[0;32m~/git/reinvent_2024_stuff/ragas-lab/.venv/lib/python3.12/site-packages/botocore/credentials.py:2074\u001b[0m, in \u001b[0;36mCredentialResolver.load_credentials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m provider \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproviders:\n\u001b[1;32m   2073\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooking for credentials via: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, provider\u001b[38;5;241m.\u001b[39mMETHOD)\n\u001b[0;32m-> 2074\u001b[0m     creds \u001b[38;5;241m=\u001b[39m \u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m creds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2076\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m creds\n",
      "File \u001b[0;32m~/git/reinvent_2024_stuff/ragas-lab/.venv/lib/python3.12/site-packages/botocore/credentials.py:1000\u001b[0m, in \u001b[0;36mProcessProvider.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m credential_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m creds_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve_credentials_using\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcredential_process\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m creds_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpiry_time\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RefreshableCredentials\u001b[38;5;241m.\u001b[39mcreate_from_metadata(\n\u001b[1;32m   1003\u001b[0m         creds_dict,\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve_credentials_using(credential_process),\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMETHOD,\n\u001b[1;32m   1006\u001b[0m     )\n",
      "File \u001b[0;32m~/git/reinvent_2024_stuff/ragas-lab/.venv/lib/python3.12/site-packages/botocore/credentials.py:1024\u001b[0m, in \u001b[0;36mProcessProvider._retrieve_credentials_using\u001b[0;34m(self, credential_process)\u001b[0m\n\u001b[1;32m   1022\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CredentialRetrievalError(\n\u001b[1;32m   1025\u001b[0m         provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMETHOD, error_msg\u001b[38;5;241m=\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1026\u001b[0m     )\n\u001b[1;32m   1027\u001b[0m parsed \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mjson\u001b[38;5;241m.\u001b[39mloads(stdout\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1028\u001b[0m version \u001b[38;5;241m=\u001b[39m parsed\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVersion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<Version key not provided>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mCredentialRetrievalError\u001b[0m: Error when retrieving credentials from custom-process: You need to authenticate with Midway. \nRun the following command before retrying: mwinit --aea\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config\n",
    "                              )\n",
    "\n",
    "kbs = [cohere_embed_hierarchical_knowledge_base, cohere_embed_semantic_knowledge_base, cohere_embed_fixed_knowledge_base] \n",
    "\n",
    "llm_for_text_generation = ChatBedrock(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\", client=bedrock_runtime_client)\n",
    "\n",
    "llm_for_evaluation = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", client=bedrock_runtime_client)\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"cohere.embed-english-v3\",client=bedrock_runtime_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve API: Process flow \n",
    "\n",
    "Create a `AmazonKnowledgeBasesRetriever` object from LangChain which will call the `Retrieve API` provided by Knowledge Bases for Amazon Bedrock which converts user queries into\n",
    "embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom\n",
    "workﬂows on top of the semantic search results. The output of the `Retrieve API` includes the the `retrieved text chunks`, the `location type` and `URI` of the source data, as well as the relevance `scores` of the retrievals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three retrievers with the knowledge base IDs\n",
    "retrievers = [\n",
    "    AmazonKnowledgeBasesRetriever(\n",
    "        knowledge_base_id=kb['knowledgeBaseId'],\n",
    "        retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 15}},\n",
    "        # endpoint_url=endpoint_url,\n",
    "        # region_name=\"us-east-1\",\n",
    "        # credentials_profile_name=\"<profile_name>\",\n",
    "    )\n",
    "    for kb in kbs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Evaluation Data\n",
    "\n",
    "As RAGAS aims to be a reference-free evaluation framework, the required preparations of the evaluation dataset are minimal. You will need to prepare `question` and `ground_truths` pairs from which you can prepare the remaining information through inference as shown below. If you are not interested in the `context_recall` metric, you don’t need to provide the `ground_truths` information. In this case, all you need to prepare are the `questions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\n",
    "    \"What was the primary reason for the increase in net cash provided by operating activities for Octank Financial in 2021?\",\n",
    "    \"In which year did Octank Financial have the highest net cash used in investing activities, and what was the primary reason for this?\",\n",
    "    \"What was the primary source of cash inflows from financing activities for Octank Financial in 2021?\",\n",
    "    \"Calculate the year-over-year percentage change in cash and cash equivalents for Octank Financial from 2020 to 2021.\",\n",
    "    \"Based on the information provided, what can you infer about Octank Financial's overall financial health and growth prospects?\"\n",
    "]\n",
    "ground_truth = [\n",
    "    \"The increase in net cash provided by operating activities was primarily due to an increase in net income and favorable changes in operating assets and liabilities.\",\n",
    "    \"Octank Financial had the highest net cash used in investing activities in 2021, at $360 million, compared to $290 million in 2020 and $240 million in 2019. The primary reason for this was an increase in purchases of property, plant, and equipment and marketable securities.\",\n",
    "    \"The primary source of cash inflows from financing activities for Octank Financial in 2021 was an increase in proceeds from the issuance of common stock and long-term debt.\",\n",
    "    \"To calculate the year-over-year percentage change in cash and cash equivalents from 2020 to 2021: \\\n",
    "    2020 cash and cash equivalents: $350 million \\\n",
    "    2021 cash and cash equivalents: $480 million \\\n",
    "    Percentage change = (2021 value - 2020 value) / 2020 value * 100 \\\n",
    "    = ($480 million - $350 million) / $350 million * 100 \\\n",
    "    = 37.14% increase\",\n",
    "    \"Based on the information provided, Octank Financial appears to be in a healthy financial position and has good growth prospects. The company has consistently increased its net cash provided by operating activities, indicating strong profitability and efficient management of working capital. Additionally, Octank Financial has been investing in long-term assets, such as property, plant, and equipment, and marketable securities, which suggests plans for future growth and expansion. The company has also been able to finance its growth through the issuance of common stock and long-term debt, indicating confidence from investors and lenders. Overall, Octank Financial's steady increase in cash and cash equivalents over the past three years provides a strong foundation for future growth and investment opportunities.\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "for retriever in retrievers:\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    kb_id = retriever.knowledge_base_id\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_for_text_generation, retriever=retriever, return_source_documents=True\n",
    "    )\n",
    "    for query in questions:\n",
    "      answers.append(qa_chain.invoke(query)[\"result\"])\n",
    "      contexts.append([docs.page_content for docs in retriever.invoke(query)])\n",
    "    # To dict\n",
    "    data[kb_id] = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": ground_truth\n",
    "    }\n",
    "\n",
    "\n",
    "datasets = {}\n",
    "for kb in kbs:\n",
    "    datasets[kb['knowledgeBaseId']] = Dataset.from_dict(data[kb['knowledgeBaseId']])\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the RAG application\n",
    "First, import all the metrics you want to use from `ragas.metrics`. Then, you can use the `evaluate()` function and simply pass in the relevant metrics and the prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import (\n",
    "harmfulness, \n",
    "maliciousness, \n",
    "coherence, \n",
    "correctness, \n",
    "conciseness\n",
    ")\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        context_entity_recall,\n",
    "        answer_similarity,\n",
    "        answer_correctness,\n",
    "        harmfulness, \n",
    "        maliciousness, \n",
    "        coherence, \n",
    "        correctness, \n",
    "        conciseness\n",
    "    ]\n",
    "\n",
    "dfs = {}\n",
    "for kb in kbs:\n",
    "    result = evaluate(\n",
    "        dataset=datasets[kb['knowledgeBaseId']],\n",
    "        metrics=metrics,\n",
    "        llm=llm_for_evaluation,\n",
    "        embeddings=bedrock_embeddings, \n",
    "    )\n",
    "    dfs[kb['knowledgeBaseId']] = result.to_pandas()\n",
    "    \n",
    "# disregard any \"failed to parse output\" errors. Those originate from the Ragas library    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "numeric_cols = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall',\n",
    "                'context_entity_recall', 'answer_similarity', 'answer_correctness', 'harmfulness',\n",
    "                'maliciousness', 'coherence', 'correctness', 'conciseness']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Calculate the mean of numeric columns for each DataFrame\n",
    "means = []\n",
    "kb_names = []\n",
    "for key, df in dfs.items():\n",
    "    mean_values = df[numeric_cols].mean()\n",
    "    means.append(mean_values)\n",
    "    \n",
    "    # Find the corresponding KB name from the 'kbs' list\n",
    "    kb_name = next((kb['name'] for kb in kbs if kb['knowledgeBaseId'] == key), 'Unknown')\n",
    "    kb_names.append(kb_name)\n",
    "\n",
    "# Combine the means into a single DataFrame\n",
    "means_df = pd.DataFrame(means, columns=numeric_cols)\n",
    "means_df = means_df.set_index(pd.Series(kb_names))\n",
    "\n",
    "# Reshape the DataFrame for plotting\n",
    "means_df = means_df.reset_index().melt(id_vars='index', value_vars=numeric_cols, var_name='Metric', value_name='Mean')\n",
    "\n",
    "# Plot the means as a bar chart\n",
    "bar_plot = means_df.set_index(['Metric', 'index'])['Mean'].unstack().plot(kind='bar', ax=ax, figsize=(20, 6), width=0.8, legend=True)\n",
    "\n",
    "# Set the chart title and axis labels\n",
    "ax.set_title('Ragas Performance', fontsize=14)\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Average', fontsize=12)\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Update the legend labels with KB names\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, kb_names, loc='upper right', ncol=1)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Please note the scores above gives a relative idea on the performance of your RAG application and should be used with caution and not as standalone scores. Also note, that we have used only 5 question/answer pairs for evaluation, as best practice, you should use enough data to cover different aspects of your document for evaluating model.\n",
    "\n",
    "If you made it this far, congrats! You have completed the workshop! If you have extra time feel free to tweak the chunking strategies further and rerunning the analysis to view your results. "
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
